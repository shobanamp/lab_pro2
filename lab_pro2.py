# -*- coding: utf-8 -*-
"""LAB_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4Ybz9rLm-6mq1bQkZP-DL4GasyG7sMI
"""

import urllib.request

url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
urllib.request.urlretrieve(url, "titanic.csv")

from pyspark.sql import SparkSession

student_id = "CH24M567"  # your roll number, letters must be in capital letter
app_name = student_id + "Project"


spark = SparkSession.builder \
    .appName(app_name) \
    .master("local[*]") \
    .config("spark.executor.instances", "1") \
    .config("spark.executor.memory", "2G") \
    .config("spark.executor.cores", "2") \
    .config("spark.driver.memory", "3G") \
    .getOrCreate()

df = spark.read.csv("titanic.csv", header=True, inferSchema=True)
df.show(5)

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer

# Step 1: Drop unnecessary columns
df = df.drop("Name", "Ticket", "Cabin")
# Step 2: Handle missing values (numerical)
imputer = Imputer(
    inputCols=["Age", "Fare"],
    outputCols=["Age_imputed", "Fare_imputed"]
)
# Step 3: Encode categorical columns
sex_indexer = StringIndexer(inputCol="Sex", outputCol="SexIndex", handleInvalid="keep")
embarked_indexer = StringIndexer(inputCol="Embarked", outputCol="EmbarkedIndex", handleInvalid="keep")

encoder = OneHotEncoder(
    inputCols=["SexIndex", "EmbarkedIndex"],
    outputCols=["SexVec", "EmbarkedVec"]
)
# Step 4: Assemble features
assembler = VectorAssembler(
    inputCols=["Pclass", "Age_imputed", "SibSp", "Parch", "Fare_imputed", "SexVec", "EmbarkedVec"],
    outputCol="features"
)
# Step 5: Build Pipeline
pipeline = Pipeline(stages=[imputer, sex_indexer, embarked_indexer, encoder, assembler])
# Apply pipeline
processed_df = pipeline.fit(df).transform(df)
# Final dataset with label
final_df = processed_df.select("features", col("Survived").alias("label"))
# Show result
final_df.show(5, truncate=False)

#from google.colab import files

final_df.toPandas().to_csv("processed_titanic.csv", index=False)
#files.download("processed_titanic.csv")
print("File saved as processed_titanic.csv successfully!")

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)
lr = LogisticRegression(featuresCol="features", labelCol="label")
lr_model = lr.fit(train_data)
predictions = lr_model.transform(test_data)
evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction")
accuracy = evaluator.evaluate(predictions)
print("Test set ROC AUC:", accuracy)

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 1.0]).build()
crossval = CrossValidator(estimator=lr,
                         estimatorParamMaps=paramGrid,
                         evaluator=evaluator,
                         numFolds=5)
cvModel = crossval.fit(train_data)
bestModel = cvModel.bestModel
print("Best ROC AUC:", evaluator.evaluate(bestModel.transform(test_data)))

import os
os.system("pip install mlflow")
print("A")

import mlflow
import mlflow.spark
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import pandas as pd
from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from mlflow.tracking import MlflowClient
import numpy as np

# List of models and corresponding hyperparameter grids
model_specs = [
    ("LogisticRegression",
        LogisticRegression(featuresCol="features", labelCol="label"),
        ParamGridBuilder()
            .addGrid(LogisticRegression.regParam, [0.01, 0.1, 1.0])
            .addGrid(LogisticRegression.elasticNetParam, [0.0, 0.5, 1.0])
            .build()
    ),
    ("DecisionTree",
        DecisionTreeClassifier(featuresCol="features", labelCol="label"),
        ParamGridBuilder()
            .addGrid(DecisionTreeClassifier.maxDepth, [3, 5, 10])
            .build()
    ),
    ("RandomForest",
        RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=50),
        ParamGridBuilder()
            .addGrid(RandomForestClassifier.numTrees, [10, 50])
            .addGrid(RandomForestClassifier.maxDepth, [3, 5, 10])
            .build()
    )
]

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("Titanic_MultiModel_HPTuning")
evaluator = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction")

best_auc = 0
best_name = None
best_model = None
best_run_id = None

input_features = ["Pclass", "Age_imputed", "SibSp", "Parch", "Fare_imputed", "SexVec", "EmbarkedVec"]

for name, model, paramGrid in model_specs:
    with mlflow.start_run(run_name=name) as run:
        mlflow.log_param("model_type", name)

        # Hyperparameter tuning via cross-validation
        crossval = CrossValidator(
            estimator=model,
            estimatorParamMaps=paramGrid,
            evaluator=evaluator,
            numFolds=5
        )
        cvModel = crossval.fit(train_data)
        bestModel = cvModel.bestModel

        predictions = bestModel.transform(test_data)
        auc = evaluator.evaluate(predictions)
        mlflow.log_metric("best_cv_roc_auc", auc)
        mlflow.spark.log_model(bestModel, "best_cv_model")

        # Log best hyperparameters found
        try:
            if name == "LogisticRegression":
                mlflow.log_param("best_regParam", bestModel._java_obj.getRegParam())
                mlflow.log_param("best_elasticNetParam", bestModel._java_obj.getElasticNetParam())
            elif name == "DecisionTree":
                mlflow.log_param("best_maxDepth", bestModel._java_obj.getMaxDepth())
            elif name == "RandomForest":
                mlflow.log_param("best_maxDepth", bestModel._java_obj.getMaxDepth())
                mlflow.log_param("best_numTrees", bestModel._java_obj.getNumTrees())
        except Exception as e:
            print(f"Could not log hyperparameters: {e}")

        # Confusion matrix
        preds_pd = predictions.select("label", "prediction").toPandas()
        cm = confusion_matrix(preds_pd["label"], preds_pd["prediction"])
        plt.figure(figsize=(6,5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title(f"Confusion Matrix - {name}")
        fname_cm = f"confusion_matrix_{name}.png"
        plt.savefig(fname_cm)
        plt.close()
        mlflow.log_artifact(fname_cm)

        # Feature importance/coefficients
        plt.figure(figsize=(8,4))
        if name == "LogisticRegression":
            coefs = bestModel.coefficients.toArray()
            plt.bar(range(len(coefs)), np.abs(coefs))
            #plt.xticks(range(len(coefs)), input_features, rotation=45)
            plt.ylabel("Coefficient Magnitude")
            plt.title("Feature Importance (LR)")
        elif name == "DecisionTree":
            importances = bestModel.featureImportances.toArray()
            plt.bar(range(len(importances)), importances)
            #plt.xticks(range(len(importances)), input_features, rotation=45)
            plt.ylabel("Importance")
            plt.title("Feature Importance (DT)")
        elif name == "RandomForest":
            importances = bestModel.featureImportances.toArray()
            plt.bar(range(len(importances)), importances)
            #plt.xticks(range(len(importances)), input_features, rotation=45)
            plt.ylabel("Importance")
            plt.title("Feature Importance (RF)")
        fname_imp = f"feature_importance_{name}.png"
        plt.tight_layout()
        plt.savefig(fname_imp)
        plt.show()
        plt.close()
        mlflow.log_artifact(fname_imp)

        print(f"{name}: best ROC AUC (CV test) = {auc}")
        # Track best model
        if auc > best_auc:
            best_auc = auc
            best_name = name
            best_model = bestModel
            best_run_id = run.info.run_id

# === Register and promote the best tuned model ===
try:
   model_uri = f"runs:/{best_run_id}/best_cv_model"
   result = mlflow.register_model(model_uri, "TitanicBestModel")
   client = MlflowClient()
   client.transition_model_version_stage(
    name="TitanicBestModel",
    version=result.version,
    stage="Production"
)

except Exception as e:
   print(f"Could not register model: {e}")

print(f"Best model ({best_name}, AUC={best_auc}) registered as TitanicBestModel and promoted to Staging.")

import os
print(os.listdir())
# or specifically:
print(os.listdir("mlruns"))

#from google.colab import files

# Download individual artifact
#files.download('feature_importance_DecisionTree.png')
#files.download('feature_importance_RandomForest.png')